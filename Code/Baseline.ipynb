{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with Simple Model\n",
    "\n",
    "In this notebook, we tried simple models like logistic regression and naive Bayes with TF-IDF and bag of words using stratified k-fold cross-validation for training nd the GridSearchCrossValidation to find the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "We used pandas to read the datasets, pandarallel for parallel processing of the dataset, and scikit-learn for the Naive Bayes and Logistic Regression models to split the dataset and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset\n",
    "This function **ast.literal_eval** is used to convert strings into Python objects, because when we load the cleaned dataset, its contents appeared as strings. We used the preprocessed dataset, created by the first data exploration and preprocess notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRew=pd.read_csv('../Dataset/datiClean.csv')\n",
    "dataMovie=pd.read_csv('../Dataset/movieclean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRew[\"clean_review\"]=dataRew.loc[:,\"clean_review\"].parallel_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMovie[\"plot_clean\"]=dataMovie.loc[:,\"plot_clean\"].parallel_apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMovie.drop(['plot_synopsis','plot_summary'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset\n",
    "\n",
    "Let's divide the dataset into train and test sets, ensuring that the sets are balanced, we set the same random state in each notebook to ensure consistent division and facilitate better result comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop the useless field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRew.drop(['review_date','movie_id','user_id','rating','review_summary','review_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataRew['clean_review']\n",
    "y=dataRew['is_spoiler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratify balance the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test dataset follow the spoiler distribution of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistic Regression and Naive Bayes, with k fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the result of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean():\n",
    "    # Calculate the averages of the metrics\n",
    "    mean_accuracy = np.mean(metrics['accuracy'])\n",
    "    mean_precision = np.mean(metrics['precision'])\n",
    "    mean_recall = np.mean(metrics['recall'])\n",
    "    mean_f1_score = np.mean(metrics['f1_score'])\n",
    "\n",
    "    # Print the averages of the metrics\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n",
    "    print(\"Mean Precision:\", mean_precision)\n",
    "    print(\"Mean Recall:\", mean_recall)\n",
    "    print(\"Mean F1 Score:\", mean_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test(y_pred_test):\n",
    "    # Calcolo delle metriche di valutazione sul set di test\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "    # Stampa delle metriche di valutazione sul set di test\n",
    "    print(\"Test Accuracy:\", accuracy_test)\n",
    "    print(\"Test Precision:\", precision_test)\n",
    "    print(\"Test Recall:\", recall_test)\n",
    "    print(\"Test F1 Score:\", f1_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model, which takes as input the number of folds and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "def computeLogistic(folds,iter,X,y_train,penalty,C,class_weight):\n",
    "    logistic_reg=LogisticRegression(max_iter=iter,penalty=penalty,C=C,class_weight=class_weight)\n",
    "    ## Stratified k-fold grant me a balance division of classes\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    for train_index, val_index in kf.split(X, y_train):\n",
    "        X_fold_train, X_fold_val = X[train_index], X[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Training\n",
    "        logistic_reg.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Validation\n",
    "        y_pred = logistic_reg.predict(X_fold_val)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_fold_val, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_fold_val, y_pred))\n",
    "        metrics['recall'].append(recall_score(y_fold_val, y_pred))\n",
    "        metrics['f1_score'].append(f1_score(y_fold_val, y_pred))\n",
    "    return logistic_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model, which takes as input the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "def compute_naive(folds,X,y_train,alpha,fit_prior):\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    naive_bayes = MultinomialNB(alpha=alpha,fit_prior=fit_prior)\n",
    "    \n",
    "    for train_index, val_index in kf.split(X, y_train):\n",
    "        X_fold_train, X_fold_val = X[train_index], X[val_index]\n",
    "        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Training\n",
    "        naive_bayes.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Validation\n",
    "        y_pred = naive_bayes.predict(X_fold_val)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_fold_val, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_fold_val, y_pred))\n",
    "        metrics['recall'].append(recall_score(y_fold_val, y_pred))\n",
    "        metrics['f1_score'].append(f1_score(y_fold_val, y_pred))\n",
    "    return naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Bag of Words\n",
    "To apply Bag of Words, first reconstruct a dummy text from tokens, then apply the function. As a result we have a matrix where each row corresponds to a document and each column corresponds to a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\" \".join(word) for word in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textT=[\" \".join(word) for word in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of Words for train\n",
    "vect=CountVectorizer()\n",
    "X=vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of Words for test\n",
    "X_t=vect.transform(textT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.values\n",
    "y_test=y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search \n",
    "Grid Search to find the regularization parameters for logistic regression and the class weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {         \n",
    "    'penalty': ['l1', 'l2'],                     # Regolarization type\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],         # Parameters for Reg\n",
    "    'class_weight': [None, 'balanced']           # Weight for Classes\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'C': 0.01, 'class_weight': None, 'penalty': 'l2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the output of the logistic regression\n",
    "\n",
    "with open(\"../Output/outputGridLog.txt\", \"a\") as f:\n",
    "    print(f\" Best Parameters:{best_params}\",file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=0.01\n",
    "class_weight=None\n",
    "penalty='l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg=computeLogistic(5,1500,X,y_train,penalty,C,class_weight)\n",
    "\n",
    "print_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logistic_reg.predict(X_t)\n",
    "print_test(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for parameters\n",
    "For Naive Bayes, we look for the smoothing parameter and the parameter that indicates whether to calculate the prior probability or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid search\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRY GRID\n",
    "naive_bayes = MultinomialNB()\n",
    "# Eseguire la ricerca su griglia\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Validation, Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes=compute_naive(5,X,y_train,best_params['alpha'],best_params['fit_prior'])\n",
    "print_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = naive_bayes.predict(X_t)\n",
    "\n",
    "print_test(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(textT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for tf-idf try different regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TF-IDF, keep the results from the previous grid search and only changes the parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {                        \n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],         # Parameters for Reg\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(max_iter=1500,penalty='l2')\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1\n",
    "penalty=\"l2\"\n",
    "class_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg=computeLogistic(5,1500,X_train_tfidf,y_train,penalty,C,class_weight)\n",
    "print_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logistic_reg.predict(X_test_tfidf)\n",
    "print_test(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid search\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRY GRID\n",
    "naive_bayes = MultinomialNB()\n",
    "# Eseguire la ricerca su griglia\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes=compute_naive(5,X_train_tfidf,y_train,best_params['alpha'],best_params['fit_prior'])\n",
    "print_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = naive_bayes.predict(X_test_tfidf)\n",
    "\n",
    "print_test(y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
