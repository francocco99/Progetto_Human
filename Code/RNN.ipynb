{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a6087d-6269-4704-a254-e920302e4161",
   "metadata": {},
   "source": [
    "# RNN\n",
    "We used an RNN, this time on the preprocessed text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632899a-43ec-4a56-8ab7-7cbfda0261e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb639e8-ea1b-4a81-9623-b577edfa149f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d89ff-7698-463a-beca-cd9e4025f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a732852-8879-4969-85e0-124f78473f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense,Embedding, Bidirectional, Attention, LSTM, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4414f-5852-462e-a703-d0fe892831e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974b777-ee4d-4eeb-8c82-e68e0f04340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13d0f8-108e-420a-8899-204563a7db7d",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1cf7a-2136-474d-86fc-746c26e98500",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK GPU\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24c5ad-aaf6-4354-8641-19a5b3c2ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952b0b0-4eb8-4806-8995-33ab9e63e578",
   "metadata": {},
   "source": [
    "### Functions\n",
    "The first function serves to convert a string into a Python object, the second to create text from tokens, and the last to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350537c-20f4-4443-a852-3760d99e0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS DEFINITION\n",
    "\n",
    "#READ SPLIT TOKENS\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing value {val}: {e}\")\n",
    "        return val  # Return the original value if there is an error\n",
    "\n",
    "#MERGE TOKENS AS A WHOLE TEXT\n",
    "def join_tokens(token_list):\n",
    "    if isinstance(token_list, list):\n",
    "        return ' '.join(token_list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "#SPLIT TRAIN + TEST 80-20\n",
    "def split_train_test(df, label_name):\n",
    "    train, test= train_test_split(df, test_size=0.2, stratify=df[label_name],random_state=42)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d52b4-ebd9-43f8-83c4-ac08c84be47d",
   "metadata": {},
   "source": [
    "###  Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743baa8-3261-49a3-81a6-13bc04c42c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData=pd.read_csv(\"../Dataset/datiClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb878e4-99bf-4d45-9c25-611d9ca6d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData=CleanData[[\"clean_review\",\"is_spoiler\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b6c2b-ad3c-4340-91a2-602c8590a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb9992-8855-4ca4-8b12-68517dd33229",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = CleanData['is_spoiler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05a7cb-1bdd-4c54-bb9c-9ba871002ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData[\"clean_review\"] = CleanData[\"clean_review\"].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6db5b-5f87-40b5-8dd5-358a2f274fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9556f90-5978-497d-87d5-5f9516c6d8a2",
   "metadata": {},
   "source": [
    "### Compute the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d9458-5444-42ef-ab63-6b00d8264da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af4159-cb74-4770-a3ac-aedfb763904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a585e-5da0-4c36-957e-f0696bc4f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in CleanData[\"clean_review\"]:\n",
    "    token_counts.update(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102306b-dbf2-41bd-9ad0-65a1a997ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41926a3a-6d58-497a-9bd3-040afa2c064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that occur less than the unk_cutoff\n",
    "vocab = {token: count for token, count in token_counts.items() if count >= 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a702a32-3781-4c06-8698-8aebcf38af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e33c3a-afc2-4055-8821-ddb73158f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2886d0b-756d-464a-84ef-a1bc074ecb57",
   "metadata": {},
   "source": [
    "### Divide in Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec57fb1-b3c4-4bf1-9fe2-52efc41887c6",
   "metadata": {},
   "source": [
    "Create a dummy text thanks to the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781a328-96b2-4ec3-a6a0-05df3400a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData[\"whole__text\"] = CleanData[\"clean_review\"].apply(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8ed58-016a-4669-b1cb-7ba41f3ffeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = CleanData['whole__text']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be68b2a-3188-4319-a9e0-ab093b7a193e",
   "metadata": {},
   "source": [
    "map the Boolean values in values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b89b25-585b-4b06-82d8-3e589af797cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData['is_spoiler_numeric'] = np.where(CleanData['is_spoiler'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181ddae-a300-4b00-b90f-e53a65fbb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData = CleanData.rename(columns={'is_spoiler_numeric': 'label','whole__text':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687f4d3-b467-4b54-88e0-19a0edc957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(CleanData, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289031f-6aa0-4e8c-8d58-a14aca880b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['text','label']]\n",
    "test = test[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962ed85-e197-4139-b8db-40e4c46ae49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a68017-4915-449f-bf58-77d4e7d5fe84",
   "metadata": {},
   "source": [
    "### Transform the dataset\n",
    "\n",
    "Let's transform the pandas dataset into a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfbb86-c7fa-44d9-b607-3b9962f84ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(train['text'].values, tf.string),\n",
    "            tf.cast(train['label'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d6cc7-a90d-42de-bfc4-cad08e827e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset =( \n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(test['text'].values, tf.string),\n",
    "            tf.cast(test['label'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0215d-98b2-4c0b-992a-44f811a4ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b48dc-1ec8-4f3f-91c5-ab187eba6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in training_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4272d-a6f6-44be-b03a-437109812dc2",
   "metadata": {},
   "source": [
    "## RNN MODEL\n",
    "Next, we define the model with its various layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc22521-aeb3-4a13-928b-95bec00d0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1e298-a033-4164-9d66-99c7e0178e2f",
   "metadata": {},
   "source": [
    "\n",
    "This code prepares the training and test datasets for training machine learning models using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e8092-eb33-4ae0-8c69-6dd78618a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74bc77-e151-4e0e-8604-f345f05cad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e8c41-1dc3-4b2a-88df-d8c1fb3c4bd4",
   "metadata": {},
   "source": [
    "\n",
    "Once adapted, the encoder can be used to convert textual input data into numerical tensors that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce79ff-4e90-4311-9f51-eeab074b6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(training_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56b960-103e-4ce6-a94e-bde3c6700eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4256916-2a62-4c0c-9a16-b6071580c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=256,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b22c5-8805-4875-b4b7-9701f36c0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4,weight_decay=0.02),\n",
    "             metrics=['accuracy','recall','precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073aa9a1-d404-439b-91e0-5163dfbea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334da22-1a4f-492c-9ee5-e61b0b58c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_dataset, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc04a7-2054-4982-8370-0ebcf36e7918",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782a571-0f88-45cf-b845-ae13bda8e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy values for each epoch:\")\n",
    "for epoch in range(len(history.history['accuracy'])):\n",
    "    accuracy = history.history['accuracy'][epoch]\n",
    "    precision = history.history['precision'][epoch]\n",
    "    recall = history.history['recall'][epoch]\n",
    "    f1_score=2* (precision * recall) / (precision + recall)\n",
    "    #f1_score = history.history['f1_m'][epoch]\n",
    "    print(f\"Epoch {epoch}: {accuracy} \" f\"recall: {recall} \" f\"precision: {precision} \" f\"f1-score: {f1_score} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab53a3-303b-425a-9d30-b915bbaf4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_mean = np.mean(history.history['accuracy'])\n",
    "training_precision_mean = np.mean(history.history['precision'])\n",
    "training_recall_mean = np.mean(history.history['recall'])\n",
    "\n",
    "\n",
    "print(f\"    Media Training accuracy: {training_accuracy_mean}\")\n",
    "print(f\"    Media Training precision: {training_precision_mean}\")\n",
    "print(f\"    Media Training recall: {training_recall_mean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e990d0c-263a-4792-aaa5-9aa7b8ae6ca2",
   "metadata": {},
   "source": [
    "### Result obtained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b3aea-f7c9-45ab-ad37-c813e70aadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTest=model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e157c-0532-4ce1-b926-24b9f4540a36",
   "metadata": {},
   "source": [
    "Save the result on a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0fb31-f656-4f24-8e3b-002689b3348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Output/outputRNN.txt\", \"a\") as f:\n",
    "   for epoch in range(len(history.history['accuracy'])):\n",
    "        accuracy = history.history['accuracy'][epoch]\n",
    "        precision = history.history['precision'][epoch]\n",
    "        recall = history.history['recall'][epoch]\n",
    "        f1_score=2* (precision * recall) / (precision + recall)\n",
    "        print(f\"Epoch {epoch}: {accuracy} \" f\"recall: {recall} \" f\"precision: {precision} \" f\"f1-score: {f1_score} \",file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9b8ea-8aa3-4fad-b506-c2162c711d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Output/outputRNN.txt\", \"a\") as f:\n",
    "    print(\"Test Result\",file=f)\n",
    "    precision=resultTest[3]\n",
    "    recall=resultTest[2]\n",
    "    f1_score=2* (precision * recall) / (precision + recall)\n",
    "    print(f\"  Loss: {resultTest[0]}, Accuracy: {resultTest[1]}, F1: {f1_score}, Precision: {resultTest[3]}, Recall: {resultTest[2]}\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c10bac-fc2e-461b-8827-74831e92f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080cdfa0-c8bc-40e8-82e0-a3be0c1728ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Loss: {resultTest[0]}, Accuracy: {resultTest[1]}, F1: {f1_score}, Precision: {resultTest[3]}, Recall: {resultTest[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd8913-81a7-41ab-b3a5-b046a2745909",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add82115-4a98-4875-83ea-4e7551b0971f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
